library("rvest")
library("plyr")
library("stringr")
# Link & css-selectors
wikipedia.link = "https://en.wikipedia.org/wiki/List_of_House_members_of_the_42nd_Parliament_of_Canada"
css.con.party = "td:nth-child(2) a"
css.con.name = "td:nth-child(3)"
# Get page
cons.name = read_html(wikipedia.link, encoding = "UTF-8") %>%
html_nodes(css = css.con.name) %>%
html_text()
cons.party = read_html(wikipedia.link, encoding = "UTF-8") %>%
html_nodes(css = css.con.party) %>%
html_text()
# Delete wikipedia header - first 3 elements in list in order to have list of same lenght for bind
cons.party = cons.party[-c(1:3)]
# Coerce intro dataframe and 'grep' only Conservatives
cons.wiki = data.frame(c(cons.party), c(cons.name))
cons.wiki = cons.wiki[grep("\\Conservative", cons.wiki$c.cons.name),]
# Separate names - names of length 2+ will be taken care of later
cons.wiki$fname = word(cons.wiki$c.cons.party., +1)
cons.wiki$lname = word(cons.wiki$c.cons.party., -1)
# The link-type:
cons.wiki$link = "http://www.conservative.ca/team/member/?fname=LANK&lname=LAKN&type=mps"
# Define vectors of each name
fname = cons.wiki$fname
lname = cons.wiki$lname
# .
c.wiki.list = cons.wiki$link[1]
cons.link.df = read.csv("https://raw.githubusercontent.com/adamingwersen/CA/master/ConsLearn.csv", header= FALSE, sep = ";")
cons.link.df = cons.link.df[-grep("Erin O'Toole", cons.link.df$V1),]
cons.link.df = cons.link.df[-grep("Dave MacKenzie", cons.link.df$V1),]
cons.link.df = cons.link.df[-grep("Peter Van Loan", cons.link.df$V1),]
# Reformat to vector for smooth looping..
cons.link.li = list(cons.link.df$V2)
cons.link.li = unlist(cons.link.df$V2)
css.selector.fb_page = ".aside-meta-block:nth-child(1)"
require("rvest")
scrape_fb_con = function(cons.link.li) {
fb.get.con = read_html(cons.link.li, encoding = "UTF-8") %>%
html_nodes(css = css.selector.fb_page) %>%
html_attr(name = 'href')
return(cbind(fb.get.con))
page.link.df = list()
for(i in cons.link.li){
print(paste("processing", i, sep = " "))
page.link.df[[i]] = scrape_fb_con(i)
Sys.sleep(0.05)
cat("done\n")
}
scrape_fb_con = function(cons.link.li) {
fb.get.con = read_html(cons.link.li, encoding = "UTF-8") %>%
html_nodes(css = css.selector.fb_page) %>%
html_attr(name = 'href')
return(cbind(fb.get.con))
}
page.link.df = list()
for(i in cons.link.li){
print(paste("processing", i, sep = " "))
page.link.df[[i]] = scrape_fb_con(i)
Sys.sleep(0.05)
cat("done\n")
}
trans1.con= data.frame(page.link.df)                # -> Dataframe
trans1.con= data.frame(page.link.df)                # -> Dataframe
trans1.con = ldply(page.link.df, data.frame)
View(cons.link.df)
facebook.links.cons = as.list(trans1.con)           # -> List
facebook.links.cons = unlist(facebook.links.cons)   # Vectorize/unlist
facebook.links.tr = gsub("\\https://www.facebook.com/", "",  facebook.links.cons)   # Remove string of type 1
facebook.links.tr2 = gsub("\\http://facebook.com/", "",  facebook.links.tr)         # Remove string of type 2
facebook.links.tr3 = gsub("pmharper", "RtHonStephenHarper/",  facebook.links.tr2)   # Stephen Harper has changed - conservative.ca is out-of-date
facebook.links.tr3 = facebook.links.tr3[!duplicated(facebook.links.tr)]
load("fb_oauth")
get_fb_cons = function(facebook.links.tr3){
fb.feed.con = getPage(facebook.links.tr3, token=fb_oauth, n = 100000, since = '2015/07/01', until = '2015/10/19')
return(cbind(fb.feed.con, facebook.links.tr3))
}
fb.list.conservative = list()
for(i in facebook.links.tr3){
print(paste("processing", i, sep = " :: "))
fb.list.conservative[[i]] = get_fb_cons(i)
Sys.sleep(0.05)
cat("done!\n")
}
library("Rfacebook")
fb.list.conservative = list()
for(i in facebook.links.tr3){
print(paste("processing", i, sep = " :: "))
fb.list.conservative[[i]] = get_fb_cons(i)
Sys.sleep(0.05)
cat("done!\n")
}
View(cons.link.df)
facebook.links.tr = gsub("\\https://www.facebook.com/", "",  facebook.links.cons)   # Remove string of type 1
facebook.links.tr2 = gsub("\\http://facebook.com/", "",  facebook.links.tr)         # Remove string of type 2
facebook.links.tr3 = gsub("pmharper", "RtHonStephenHarper/",  facebook.links.tr2)   # Stephen Harper has changed - conservative.ca is out-of-date
facebook.links.tr3 = facebook.links.tr3[!duplicated(facebook.links.tr)]             # Remove any duplicates
head(page.link.df, 10)
View(cons.link.df)
cons.link.df = read.csv("https://raw.githubusercontent.com/adamingwersen/CA/master/ConsLearn.csv", header= FALSE, sep = ";")
cons.link.df = cons.link.df[-grep("Erin O'Toole", cons.link.df$V1),]
cons.link.df = cons.link.df[-grep("Dave MacKenzie", cons.link.df$V1),]
cons.link.df = cons.link.df[-grep("Peter Van Loan", cons.link.df$V1),]
cons.link.li = list(cons.link.df$V2)
cons.link.li = unlist(cons.link.df$V2)
View(trans1.con)
View(trans1.con)
facebook.links.tr = gsub("\\https://www.facebook.com/", "",  facebook.links.cons)   # Remove string of type 1
facebook.links.tr2 = gsub("\\http://facebook.com/", "",  facebook.links.tr)         # Remove string of type 2
facebook.links.tr3 = gsub("pmharper", "RtHonStephenHarper/",  facebook.links.tr2)   # Stephen Harper has changed - conservative.ca is out-of-date
facebook.links.tr3 = facebook.links.tr3[!duplicated(facebook.links.tr)]             # Remove any duplicates
View(trans1.con)
View(trans1.con)
View(trans1.con)
View(trans1.con)
library("rvest")
library("plyr")
library("stringr")
cons.link.df = read.csv("https://raw.githubusercontent.com/adamingwersen/CA/master/ConsLearn.csv", header= FALSE, sep = ";")
View(cons.link.df)
cons.link.df = cons.link.df[-grep("Erin O'Toole", cons.link.df$V1),]
cons.link.df = cons.link.df[-grep("Dave MacKenzie", cons.link.df$V1),]
cons.link.df = cons.link.df[-grep("Peter Van Loan", cons.link.df$V1),]
cons.link.li = list(cons.link.df$V2)
cons.link.li = unlist(cons.link.df$V2)
css.selector.fb_page = ".aside-meta-block:nth-child(1)"
scrape_fb_con = function(cons.link.li) {
fb.get.con = read_html(cons.link.li, encoding = "UTF-8") %>%
html_nodes(css = css.selector.fb_page) %>%
html_attr(name = 'href')
return(cbind(fb.get.con))
}
page.link.df = list()
for(i in cons.link.li){
print(paste("processing", i, sep = " "))
page.link.df[[i]] = scrape_fb_con(i)
Sys.sleep(0.05)
cat("done\n")
}
View(cons.link.df)
head(page.link.df, 10)
trans1.con = ldply(page.link.df, data.frame)
View(trans1.con)
trans1.con= data.frame(page.link.df)                # -> Dataframe
trans1.con = ldply(page.link.df, data.frame)
View(trans1.con)
facebook.links.cons = trans1.con$fb.get.con
facebook.links.cons = as.list(facebook.links.cons)           # -> List
facebook.links.cons = unlist(facebook.links.cons)   # Vectorize/unlist
View(trans1.con)
facebook.links.cons = trans1.con$fb.get.con
trans1.con = ldply(page.link.df, data.frame)
hej
trans1.con = ldply(page.link.df, data.frame)
facebook.links.cons = trans1.con$fb.get.con
facebook.links.tr = gsub("\\https://www.facebook.com/", "",  trans1.con$fb.get.con)   # Remove string of type 1
facebook.links.tr2 = gsub("\\http://facebook.com/", "",  facebook.links.tr)         # Remove string of type 2
facebook.links.tr3 = gsub("pmharper", "RtHonStephenHarper/",  facebook.links.tr2)   # Stephen Harper has changed - conservative.ca is out-of-date
facebook.links.tr3 = facebook.links.tr3[!duplicated(facebook.links.tr)]             # Remove any duplicates
save.image("~/GitHub/CA/envirCA.RData")
save.image("~/GitHub/CA/envirCA.RData")
library("Rfacebook")
library("httr")
load("fb_oauth")
get_fb_cons = function(facebook.links.tr3){
fb.feed.con = getPage(facebook.links.tr3, token=fb_oauth, n = 100000, since = '2015/07/01', until = '2015/10/19')
return(cbind(fb.feed.con, facebook.links.tr3))
}
options(warn=1)
fb.list.conservative = list()
for(i in facebook.links.tr3){
print(paste("processing", i, sep = " :: "))
fb.list.conservative[[i]] = get_fb_cons(i)
Sys.sleep(0.05)
cat("done!\n")
}
cons.link.df = read.csv("https://raw.githubusercontent.com/adamingwersen/CA/master/ConsLearn.csv", header= FALSE, sep = ";")
cons.link.df = cons.link.df[-grep("Erin O'Toole", cons.link.df$V1),]
cons.link.df = cons.link.df[-grep("Dave MacKenzie", cons.link.df$V1),]
cons.link.df = cons.link.df[-grep("Peter Van Loan", cons.link.df$V1),]
cons.link.li = list(cons.link.df$V2)
cons.link.li = unlist(cons.link.df$V2)
page.link.df = list()
for(i in cons.link.li){
print(paste("processing", i, sep = " "))
page.link.df[[i]] = scrape_fb_con(i)
Sys.sleep(0.05)
cat("done\n")
}
trans1.con = ldply(page.link.df, data.frame)
facebook.links.tr = gsub("\\https://www.facebook.com/", "",  trans1.con$fb.get.con)   # Remove string of type 1
facebook.links.tr2 = gsub("\\http://facebook.com/", "",  facebook.links.tr)         # Remove string of type 2
facebook.links.tr3 = gsub("pmharper", "RtHonStephenHarper/",  facebook.links.tr2)   # Stephen Harper has changed - conservative.ca is out-of-date
facebook.links.tr3 = facebook.links.tr3[!duplicated(facebook.links.tr)]             # Remove any duplicates
fb.list.conservative = list()
for(i in facebook.links.tr3){
print(paste("processing", i, sep = " :: "))
fb.list.conservative[[i]] = get_fb_cons(i)
Sys.sleep(0.05)
cat("done!\n")
}
cons.link.df = read.csv("https://raw.githubusercontent.com/adamingwersen/CA/master/ConsLearn.csv", header= FALSE, sep = ";")
cons.link.df = cons.link.df[-grep("Erin O'Toole", cons.link.df$V1),]
cons.link.df = cons.link.df[-grep("Dave MacKenzie", cons.link.df$V1),]
cons.link.df = cons.link.df[-grep("Peter Van Loan", cons.link.df$V1),]
cons.link.li = list(cons.link.df$V2)
cons.link.li = unlist(cons.link.df$V2)
page.link.df = list()
for(i in cons.link.li){
print(paste("processing", i, sep = " "))
page.link.df[[i]] = scrape_fb_con(i)
Sys.sleep(0.05)
cat("done\n")
}
trans1.con = ldply(page.link.df, data.frame)
facebook.links.tr = gsub("\\https://www.facebook.com/", "",  trans1.con$fb.get.con)   # Remove string of type 1
facebook.links.tr2 = gsub("\\http://facebook.com/", "",  facebook.links.tr)         # Remove string of type 2
facebook.links.tr3 = gsub("pmharper", "RtHonStephenHarper/",  facebook.links.tr2)   # Stephen Harper has changed - conservative.ca is out-of-date
facebook.links.tr3 = facebook.links.tr3[!duplicated(facebook.links.tr)]
options(warn=1)
fb.list.conservative = list()
for(i in facebook.links.tr3){
print(paste("processing", i, sep = " :: "))
fb.list.conservative[[i]] = get_fb_cons(i)
Sys.sleep(0.05)
cat("done!\n")
}
facebook.conservative.df = ldply(fb.list.conservative, data.frame)
View(facebook.conservative.df)
save.image("~/GitHub/CA/envirCA.RData")
testndp.link = "http://www.ndp.ca/team"
css.selector.ndp3 = "a.candidate-facebook"
css.selector.ndpname = "strong"
css.selector.province = "span.candidate-prov-abbrev"
ndp.link = read_html(testndp.link, encoding = "UTF-8") %>%
html_nodes(css = css.selector.ndp3) %>%
html_attr(name = 'href')
ndp.name = read_html(testndp.link, encoding = "UTF-8") %>%
html_nodes(css = css.selector.ndpname) %>%
html_text()
ndp.province = read_html(testndp.link, encoding = "UTF-8") %>%
html_nodes(css = css.selector.province) %>%
html_text()
ndp.candidate.profile.list = data.frame(ndp.name, ndp.province)
ndp.candidate.profile.list = ndp.candidate.profile.list[-grep("Daniel Blaikie", ndp.candidate.profile.list$ndp.name),]
ndp.candidate.profile.list = ndp.candidate.profile.list[-grep("Karine Trudel", ndp.candidate.profile.list$ndp.name),]
ndp.candidate.profile.list = ndp.candidate.profile.list[-grep("Gord Johns", ndp.candidate.profile.list$ndp.name),]
# Match names with links
ndp.candidate.complete.frame = data.frame(ndp.candidate.profile.list, ndp.link)
# Remove candidates with invalid facebook pages
ndp.candidate.complete.frame = ndp.candidate.complete.frame[-grep("Carol Hughes", ndp.candidate.complete.frame$ndp.name),]
ndp.candidate.complete.frame = ndp.candidate.complete.frame[-grep("Anne Minh-Thu Quach", ndp.candidate.complete.frame$ndp.name),]
ndp.candidate.complete.frame = ndp.candidate.complete.frame[-grep("charlie.angus.58", ndp.candidate.complete.frame$fbref),]
ndp.candidate.complete.frame = ndp.candidate.complete.frame[-grep("FrancoisChoquette.deputeDrummond", ndp.candidate.complete.frame$fbref),]
ndp.candidate.complete.frame = ndp.candidate.complete.frame[-grep("brianmassemp", ndp.candidate.complete.frame$fbref),]
View(ndp.candidate.profile.list)
library("stringr")
ndp.candidate.complete.frame$fbref = gsub("\\https://fb.com/", "", ndp.candidate.complete.frame$ndp.link)
# List of FB ext's
fbref.sub = ndp.candidate.complete.frame[ ,'fbref']
message(dQuote(fbref.sub))
fbref.sub = ndp.candidate.complete.frame[ ,'fbref']
ndp.candidate.complete.frame$fbref = gsub("\\https://fb.com/", "", ndp.candidate.complete.frame$ndp.link)
View(ndp.candidate.profile.list)
fbref.sub = ndp.candidate.complete.frame[ ,'fbref']
View(ndp.candidate.profile.list)
View(ndp.candidate.complete.frame)
ndp.candidate.complete.frame = data.frame(ndp.candidate.profile.list, ndp.link)
ndp.candidate.complete.frame = ndp.candidate.complete.frame[-grep("Carol Hughes", ndp.candidate.complete.frame$ndp.name),]
ndp.candidate.complete.frame = ndp.candidate.complete.frame[-grep("Anne Minh-Thu Quach", ndp.candidate.complete.frame$ndp.name),]
ndp.candidate.complete.frame = ndp.candidate.complete.frame[-grep("charlie.angus.58", ndp.candidate.complete.frame$fbref),]
ndp.candidate.complete.frame = ndp.candidate.complete.frame[-grep("FrancoisChoquette.deputeDrummond", ndp.candidate.complete.frame$fbref),]
ndp.candidate.complete.frame = ndp.candidate.complete.frame[-grep("brianmassemp", ndp.candidate.complete.frame$fbref),]
ndp.candidate.complete.frame = data.frame(ndp.candidate.profile.list, ndp.link)
ndp.candidate.complete.frame = ndp.candidate.complete.frame[-grep("Carol Hughes", ndp.candidate.complete.frame$ndp.name),]
ndp.candidate.complete.frame = ndp.candidate.complete.frame[-grep("Anne Minh-Thu Quach", ndp.candidate.complete.frame$ndp.name),]
ndp.candidate.complete.frame = ndp.candidate.complete.frame[-grep("charlie.angus.58", ndp.candidate.complete.frame$fbref),]
ndp.candidate.complete.frame = ndp.candidate.complete.frame[-grep("Carol Hughes", ndp.candidate.complete.frame$ndp.name),]
ndp.candidate.complete.frame = data.frame(ndp.candidate.profile.list, ndp.link)
ndp.candidate.complete.frame = ndp.candidate.complete.frame[-grep("Carol Hughes", ndp.candidate.complete.frame$ndp.name),]
ndp.candidate.complete.frame = ndp.candidate.complete.frame[-grep("charlie.angus.58", ndp.candidate.complete.frame$fbref),]
ndp.candidate.complete.frame = data.frame(ndp.candidate.profile.list, ndp.link)
View(ndp.candidate.complete.frame)
ndp.candidate.complete.frame = ndp.candidate.complete.frame[-grep("\\charlie.angus.58", ndp.candidate.complete.frame$fbref),]
ndp.candidate.complete.frame = data.frame(ndp.candidate.profile.list, ndp.link)
?cbind
View(ndp.candidate.complete.frame)
ndp.candidate.profile.list = ndp.candidate.profile.list[-grep("Carol Hughes", ndp.candidate.profile.list$ndp.name),]
ndp.candidate.profile.list = ndp.candidate.profile.list[-grep("Anne Minh-Thu Quach", ndp.candidate.profile.list$ndp.name),]
ndp.candidate.profile.list = ndp.candidate.profile.list[-grep("charlie.angus.58", ndp.candidate.profile.list$fbref),]
ndp.candidate.profile.list = data.frame(ndp.name, ndp.province)
ndp.candidate.profile.list = ndp.candidate.profile.list[-grep("Daniel Blaikie", ndp.candidate.profile.list$ndp.name),]
ndp.candidate.profile.list = ndp.candidate.profile.list[-grep("Karine Trudel", ndp.candidate.profile.list$ndp.name),]
ndp.candidate.profile.list = ndp.candidate.profile.list[-grep("Gord Johns", ndp.candidate.profile.list$ndp.name),]
ndp.candidate.profile.list = ndp.candidate.profile.list[-grep("Carol Hughes", ndp.candidate.profile.list$ndp.name),]
ndp.candidate.profile.list = ndp.candidate.profile.list[-grep("Anne Minh-Thu Quach", ndp.candidate.profile.list$ndp.name),]
View(ndp.candidate.complete.frame)
View(ndp.candidate.profile.list)
ndp.candidate.profile.list = ndp.candidate.profile.list[-grep("Charlie Angus", ndp.candidate.profile.list$ndp.name),]
View(ndp.candidate.complete.frame)
ndp.candidate.profile.list = ndp.candidate.profile.list[-grep("FranÃ§ois Choquette", ndp.candidate.profile.list$ndp.name),]
View(ndp.candidate.complete.frame)
View(ndp.candidate.profile.list)
ndp.candidate.profile.list = ndp.candidate.profile.list[-grep("Brian Masse", ndp.candidate.profile.list$ndp.name),]
ndp.candidate.complete.frame = data.frame(ndp.candidate.profile.list, ndp.link)
ndp.link = read_html(testndp.link, encoding = "UTF-8") %>%
html_nodes(css = css.selector.ndp3) %>%
html_attr(name = 'href')
ndp.name = read_html(testndp.link, encoding = "UTF-8") %>%
html_nodes(css = css.selector.ndpname) %>%
html_text()
ndp.province = read_html(testndp.link, encoding = "UTF-8") %>%
html_nodes(css = css.selector.province) %>%
html_text()
# Coerce into data.frame
ndp.candidate.profile.list = data.frame(ndp.name, ndp.province, ndp.link)
View(ndp.candidate.profile.list)
head(ndp.link, 50)
head(ndp.name, 50)
ndp.candidate.profile.list = ndp.candidate.profile.list[-grep("Daniel Blaikie", ndp.candidate.profile.list$ndp.name),]
ndp.candidate.profile.list = ndp.candidate.profile.list[-grep("Karine Trudel", ndp.candidate.profile.list$ndp.name),]
ndp.candidate.profile.list = ndp.candidate.profile.list[-grep("Gord Johns", ndp.candidate.profile.list$ndp.name),]
ndp.candidate.profile.list = data.frame(ndp.candidate.profile.list, ndp.link)
ndp.candidate.profile.list = data.frame(ndp.name, ndp.province, ndp.link)
ndp.candidate.profile.list = data.frame(ndp.name, ndp.province, ndp.link)
ndp.candidate.profile.list = data.frame(ndp.name, ndp.province, ndp.link)
ndp.candidate.profile.list = data.frame(ndp.name, ndp.province)
ndp.candidate.profile.list = ndp.candidate.profile.list[-grep("Daniel Blaikie", ndp.candidate.profile.list$ndp.name),]
ndp.candidate.profile.list = ndp.candidate.profile.list[-grep("Karine Trudel", ndp.candidate.profile.list$ndp.name),]
ndp.candidate.profile.list = ndp.candidate.profile.list[-grep("Gord Johns", ndp.candidate.profile.list$ndp.name),]
ndp.candidate.profile.list = data.frame(ndp.candidate.profile.list, ndp.link)
ndp.candidate.profile.list = ndp.candidate.profile.list[-grep("Carol Hughes", ndp.candidate.profile.list$ndp.name),]
ndp.candidate.profile.list = ndp.candidate.profile.list[-grep("Anne Minh-Thu Quach", ndp.candidate.profile.list$ndp.name),]
ndp.candidate.profile.list = ndp.candidate.profile.list[-grep("Charlie Angus", ndp.candidate.profile.list$ndp.name),]
ndp.candidate.profile.list = ndp.candidate.profile.list[-grep("FranÃ§ois Choquette", ndp.candidate.profile.list$ndp.name),]
ndp.candidate.profile.list = ndp.candidate.profile.list[-grep("Brian Masse", ndp.candidate.profile.list$ndp.name),]
ndp.candidate.complete.frame = ndp.profile.list
ndp.candidate.complete.frame = ndp.candidate.profile.list
View(ndp.candidate.profile.list)
ndp.candidate.complete.frame$fbref = gsub("\\https://fb.com/", "", ndp.candidate.complete.frame$ndp.link)
fbref.sub = ndp.candidate.complete.frame[ ,'fbref']
message(dQuote(fbref.sub))
library("Rfacebook")
scrape_ndp_fb = function(fbref.sub){
fbndp = getPage(fbref.sub, token = fb_oauth, n = 10000, since = '2015/07/01', until = '2015/10/19')
if(class(fbndp)=='try-error') next;
return(fbndp)
}
plyrndp = data.frame(ldply(fbref.sub, scrapendp_fb, .inform = TRUE))
plyrndp = data.frame(ldply(fbref.sub, scrape_ndp_fb, .inform = TRUE))
save.image("~/GitHub/CA/envirCA.RData")
plyrndp = data.frame(ldply(fbref.sub, scrape_ndp_fb, .inform = TRUE))
scrape_ndp_fb = function(fbref.sub){
fbndp = getPage(fbref.sub, token = fb_oauth, n = 10000, since = '2015/07/01', until = '2015/10/19')
return(cbind(fbndp))
}
plyrndp = data.frame(ldply(fbref.sub, scrape_ndp_fb, .inform = TRUE))
load("fb_oauth")
fb.democratic.list = list()
for(i in fbref.sub){
print(paste("processing", i, sep = " :: "))
fb.democratic.list[[i]] = scrape_ndp_fb(i)
Sys.sleep(0.05)
cat("done!\n")
}
fb.democratic.list = list()
for(i in fbref.sub){
print(paste("processing", i, sep = " :: "))
fb.democratic.list[[i]] = scrape_ndp_fb(i)
Sys.sleep(0.15)
cat("done!\n")
}
library("Rfacebook")
scrape_ndp_fb = function(fbref.sub){
fbndp = getPage(fbref.sub, token = fb_oauth, n = 10000, since = '2015/07/01', until = '2015/10/19')
return(cbind(fbndp, fbref.sub))
}
fb.democratic.list = list()
for(i in fbref.sub){
print(paste("processing", i, sep = " :: "))
fb.democratic.list[[i]] = scrape_ndp_fb(i)
Sys.sleep(0.15)
cat("done!\n")
}
plyrndp = data.frame(ldply(fbref.sub, scrape_ndp_fb, .inform = TRUE))
library("Rfacebook")
install.packages("roxygen2")
library("Rfacebook", lib.loc="~/R/win-library/3.2")
scrape_ndp_fb = function(fbref.sub){
fbndp = getPage(fbref.sub, token = fb_oauth, n = 10000, since = '2015/07/01', until = '2015/10/19')
return(cbind(fbndp, fbref.sub))
}
fb.democratic.list = list()
for(i in fbref.sub){
print(paste("processing", i, sep = " :: "))
fb.democratic.list[[i]] = scrape_ndp_fb(i)
Sys.sleep(0.15)
cat("done!\n")
}
save.image("~/GitHub/CA/envirCA.RData")
